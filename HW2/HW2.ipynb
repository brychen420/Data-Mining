{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "train_df = pd.read_json(data_path + 'train.json')\n",
    "test_df = pd.read_json(data_path + 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_text(df):\n",
    "    \n",
    "    text_list = []\n",
    "    for title, text in zip(df['title'], df['text']):\n",
    "        merged = title + ' ' + text\n",
    "        text_list.append(merged)\n",
    "        \n",
    "    df = df.assign(merged=text_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_merged = merge_title_text(train_df)\n",
    "test_df_merged = merge_title_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(train_df_merged['merged'])\n",
    "train_rating = list(train_df_merged['rating'])\n",
    "train_rating = [rating - 1 for rating in train_rating]\n",
    "\n",
    "test_list = list(test_df_merged['merged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_val, y_train, y_val = train_test_split(train_list, train_rating, test_size=0.1, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, word2index, N):\n",
    "    # text: sentences\n",
    "    # word2index: dict of words and coresponding indices\n",
    "    # label: label of emotion\n",
    "    # N: all data should be padded to length N\n",
    "    tokenized = word_tokenize(text)\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for text in X_train:\n",
    "    counts.update(word_tokenize(text))\n",
    "    \n",
    "for text in X_val:\n",
    "    counts.update(word_tokenize(text))\n",
    "\n",
    "word2index = {'unk': 0}\n",
    "for i, word in enumerate(counts.keys()):\n",
    "    word2index[word] = i+1\n",
    "\n",
    "\n",
    "train_encoded = [(encode(X_train[i], word2index, 12)) for i in range(len(y_train))]\n",
    "val_encoded = [(encode(X_val[i], word2index, 12)) for i in range(len(y_val))]\n",
    "\n",
    "train_x = np.array(train_encoded)\n",
    "train_y = np.array(y_train)\n",
    "val_x = np.array(val_encoded)\n",
    "val_y = np.array(y_val)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_ds = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper parameters\n",
    "src_vocab_size = len(word2index)\n",
    "dimension_model = 32\n",
    "num_layers = 5\n",
    "hidden_size = 30\n",
    "linear_hidden_size = 10\n",
    "classes = 5\n",
    "dropout = 0.2\n",
    "lr = 1e-3\n",
    "\n",
    "# Define properties and functions for our LSTM model\n",
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(src_vocab_size, dimension_model)\n",
    "        self.lstm = torch.nn.LSTM(input_size=dimension_model, hidden_size=hidden_size,\n",
    "                                  num_layers=num_layers, dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, linear_hidden_size)\n",
    "        self.linear1 = torch.nn.Linear(linear_hidden_size, classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.embed(data)\n",
    "        x, (h_n, c_n) = self.lstm(x.transpose(0, 1))\n",
    "\n",
    "        x = self.linear(x[-1])\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, \n",
      "Train Loss: 1.447,Train Acc: 31.61%,\n",
      "Val. Loss: 1.276, Val. Acc: 42.89%\n",
      "\n",
      "Epoch: 02, \n",
      "Train Loss: 1.193,Train Acc: 47.44%,\n",
      "Val. Loss: 1.161, Val. Acc: 47.31%\n",
      "\n",
      "Epoch: 03, \n",
      "Train Loss: 1.091,Train Acc: 51.29%,\n",
      "Val. Loss: 1.163, Val. Acc: 49.54%\n",
      "\n",
      "Epoch: 04, \n",
      "Train Loss: 1.038,Train Acc: 53.91%,\n",
      "Val. Loss: 1.118, Val. Acc: 49.83%\n",
      "\n",
      "Epoch: 05, \n",
      "Train Loss: 0.988,Train Acc: 56.43%,\n",
      "Val. Loss: 1.111, Val. Acc: 49.31%\n",
      "\n",
      "Epoch: 06, \n",
      "Train Loss: 0.946,Train Acc: 58.71%,\n",
      "Val. Loss: 1.124, Val. Acc: 50.75%\n",
      "\n",
      "Epoch: 07, \n",
      "Train Loss: 0.908,Train Acc: 60.76%,\n",
      "Val. Loss: 1.155, Val. Acc: 50.34%\n",
      "\n",
      "Epoch: 08, \n",
      "Train Loss: 0.870,Train Acc: 63.00%,\n",
      "Val. Loss: 1.172, Val. Acc: 50.80%\n",
      "\n",
      "Epoch: 09, \n",
      "Train Loss: 0.833,Train Acc: 64.99%,\n",
      "Val. Loss: 1.221, Val. Acc: 50.29%\n",
      "\n",
      "Epoch: 10, \n",
      "Train Loss: 0.799,Train Acc: 67.04%,\n",
      "Val. Loss: 1.252, Val. Acc: 50.77%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LSTM().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_acc = 0\n",
    "best_path = ''\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_dl, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, val_dl, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}, ')\n",
    "    print(f'Train Loss: {train_loss:.3f},Train Acc: {train_acc * 100:.2f}%,')\n",
    "    print(f'Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc * 100:.2f}%\\n')\n",
    "    \n",
    "    if best_acc < valid_acc:\n",
    "        best_acc = valid_acc\n",
    "        best_path = f\"epoch{epoch+1}_val.accuracy{valid_acc*100:.1f}%.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': valid_loss,\n",
    "        }, best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_test(text, word2index, N):\n",
    "    \n",
    "    tokenized = word_tokenize(text)\n",
    "    for i, word in enumerate(tokenized):\n",
    "        if word2index.get(word) == None:\n",
    "            tokenized[i] = 'unk'\n",
    "\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = [(encode_test(text, word2index, 10)) for text in test_list]\n",
    "\n",
    "test_x = np.array(test_encoded)\n",
    "test_ds = TensorDataset(torch.from_numpy(test_x))\n",
    "test_dl = DataLoader(test_ds, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "101\n",
      "201\n",
      "301\n",
      "401\n",
      "501\n",
      "601\n",
      "701\n",
      "801\n",
      "901\n",
      "1001\n",
      "1101\n",
      "1201\n",
      "1301\n",
      "1401\n",
      "1501\n",
      "1601\n",
      "1701\n",
      "1801\n",
      "1901\n",
      "2001\n",
      "2101\n",
      "2201\n",
      "2301\n",
      "2401\n",
      "2501\n",
      "2601\n",
      "2701\n",
      "2801\n",
      "2901\n",
      "3001\n",
      "3101\n",
      "3201\n",
      "3301\n",
      "3401\n",
      "3501\n",
      "3601\n",
      "3701\n",
      "3801\n",
      "3901\n",
      "4001\n",
      "4101\n",
      "4201\n",
      "4301\n",
      "4401\n",
      "4501\n",
      "4601\n",
      "4701\n",
      "4801\n",
      "4901\n",
      "5001\n",
      "5101\n",
      "5201\n",
      "5301\n",
      "5401\n",
      "5501\n",
      "5601\n",
      "5701\n",
      "5801\n",
      "5901\n",
      "6001\n",
      "6101\n",
      "6201\n",
      "6301\n",
      "6401\n",
      "6501\n",
      "6601\n",
      "6701\n",
      "6801\n",
      "6901\n",
      "7001\n",
      "7101\n",
      "7201\n",
      "7301\n",
      "7401\n",
      "7501\n",
      "7601\n",
      "7701\n",
      "7801\n",
      "7901\n",
      "8001\n",
      "8101\n",
      "8201\n",
      "8301\n",
      "8401\n",
      "8501\n",
      "8601\n",
      "8701\n",
      "8801\n",
      "8901\n",
      "9001\n",
      "9101\n",
      "9201\n",
      "9301\n",
      "9401\n",
      "9501\n",
      "9601\n",
      "9701\n",
      "9801\n",
      "9901\n",
      "10001\n",
      "10101\n",
      "10201\n",
      "10301\n",
      "10401\n",
      "10501\n",
      "10601\n",
      "10701\n",
      "10801\n",
      "10901\n",
      "11001\n",
      "11101\n",
      "11201\n",
      "11301\n",
      "11401\n",
      "11501\n",
      "11601\n",
      "11701\n",
      "11801\n",
      "11901\n",
      "12001\n",
      "12101\n",
      "12201\n",
      "12301\n",
      "12401\n",
      "12501\n",
      "12601\n",
      "12701\n",
      "12801\n",
      "12901\n",
      "13001\n",
      "13101\n",
      "13201\n",
      "13301\n",
      "13401\n",
      "13501\n",
      "13601\n",
      "13701\n",
      "13801\n",
      "13901\n",
      "14001\n",
      "14101\n",
      "14201\n",
      "14301\n",
      "14401\n",
      "14501\n",
      "14601\n",
      "14701\n",
      "14801\n",
      "14901\n",
      "15001\n",
      "15101\n",
      "15201\n",
      "15301\n",
      "15401\n",
      "15501\n",
      "15601\n",
      "15701\n",
      "15801\n",
      "15901\n",
      "16001\n",
      "16101\n",
      "16201\n",
      "16301\n",
      "16401\n",
      "16501\n",
      "16601\n",
      "16701\n",
      "16801\n",
      "16901\n",
      "17001\n",
      "17101\n",
      "17201\n",
      "17301\n",
      "17401\n",
      "17501\n",
      "17601\n",
      "17701\n",
      "17801\n",
      "17901\n",
      "18001\n",
      "18101\n",
      "18201\n",
      "18301\n",
      "18401\n",
      "18501\n",
      "18601\n",
      "18701\n",
      "18801\n",
      "18901\n",
      "19001\n",
      "19101\n",
      "19201\n",
      "19301\n",
      "19401\n",
      "19501\n",
      "19601\n",
      "19701\n",
      "19801\n",
      "19901\n",
      "20001\n",
      "20101\n",
      "20201\n",
      "20301\n",
      "20401\n",
      "20501\n",
      "20601\n",
      "20701\n",
      "20801\n",
      "20901\n",
      "21001\n",
      "21101\n",
      "21201\n",
      "21301\n",
      "21401\n",
      "21501\n",
      "21601\n",
      "21701\n",
      "21801\n",
      "21901\n",
      "22001\n",
      "22101\n",
      "22201\n",
      "22301\n",
      "22401\n",
      "22501\n",
      "22601\n",
      "22701\n",
      "22801\n",
      "22901\n",
      "23001\n",
      "23101\n",
      "23201\n",
      "23301\n",
      "23401\n",
      "23501\n",
      "23601\n",
      "23701\n",
      "23801\n",
      "23901\n",
      "24001\n",
      "24101\n",
      "24201\n",
      "24301\n",
      "24401\n",
      "24501\n",
      "24601\n",
      "24701\n",
      "24801\n",
      "24901\n",
      "25001\n",
      "25101\n",
      "25201\n",
      "25301\n",
      "25401\n",
      "25501\n",
      "25601\n",
      "25701\n",
      "25801\n",
      "25901\n",
      "26001\n",
      "26101\n",
      "26201\n",
      "26301\n",
      "26401\n",
      "26501\n",
      "26601\n",
      "26701\n",
      "26801\n",
      "26901\n",
      "27001\n",
      "27101\n",
      "27201\n",
      "27301\n",
      "27401\n",
      "27501\n",
      "27601\n",
      "27701\n",
      "27801\n",
      "27901\n",
      "28001\n",
      "28101\n",
      "28201\n",
      "28301\n",
      "28401\n",
      "28501\n",
      "28601\n",
      "28701\n",
      "28801\n",
      "28901\n",
      "29001\n",
      "29101\n",
      "29201\n",
      "29301\n",
      "29401\n",
      "29501\n",
      "29601\n",
      "29701\n",
      "29801\n",
      "29901\n",
      "30001\n",
      "30101\n",
      "30201\n",
      "30301\n",
      "30401\n",
      "30501\n",
      "30601\n",
      "30701\n",
      "30801\n",
      "30901\n",
      "31001\n",
      "31101\n",
      "31201\n",
      "31301\n",
      "31401\n",
      "31501\n",
      "31601\n",
      "31701\n",
      "31801\n",
      "31901\n",
      "32001\n",
      "32101\n",
      "32201\n",
      "32301\n",
      "32401\n",
      "32501\n",
      "32601\n",
      "32701\n",
      "32801\n",
      "32901\n",
      "33001\n",
      "33101\n",
      "33201\n",
      "33301\n",
      "33401\n",
      "33501\n",
      "33601\n",
      "33701\n",
      "33801\n",
      "33901\n",
      "34001\n",
      "34101\n",
      "34201\n",
      "34301\n",
      "34401\n",
      "34501\n",
      "34601\n",
      "34701\n",
      "34801\n",
      "34901\n"
     ]
    }
   ],
   "source": [
    "model = LSTM().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "checkpoint = torch.load(best_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.eval()\n",
    "\n",
    "predict = []\n",
    "for i, data in enumerate(test_dl):\n",
    "    text = data[0].to(device)\n",
    "    preds = model(text)\n",
    "    _, pred = torch.max(preds, 1)\n",
    "    predict.append(pred.item() + 1)\n",
    "    if i % 100 == 0:\n",
    "    \tprint(len(predict))\n",
    "\n",
    "indices = range(len(predict))\n",
    "indices = ['index_' + str(i) for i in indices]\n",
    "data = {\"index\": indices ,\"rating\": predict}\n",
    "out_df = pd.DataFrame(data, columns=[\"index\", \"rating\"])\n",
    "out_df.to_csv('result.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
